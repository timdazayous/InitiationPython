{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71c872c",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "**Exercice 1 - Chargement de données :**\n",
    "* Téléchargez le fichier CSV \"titanic.csv\" à partir du lien suivant : https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\n",
    "* Utilisez Pandas pour charger ce fichier dans un DataFrame nommé \"titanic_df\".\n",
    "* Affichez les 5 premières lignes du DataFrame pour inspecter les données.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faae9ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Téléchargement du fichier titanic.csv depuis un url\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "\n",
    "# Chargement du fichier dans un DataFrame titanic_df\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# Affichage des 5 premieres lignes de titanic.csv\n",
    "print(titanic_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5655f3a5",
   "metadata": {},
   "source": [
    "**Exercice 2 - Exploration des données :**\n",
    "* Trouvez le nombre total de passagers dans le Titanic.\n",
    "* Déterminez le pourcentage de passagers qui ont survécu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6c83da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le pourcentage de survivant est de 38%\n"
     ]
    }
   ],
   "source": [
    "# On recupere le nombre d'élément contenu dans titanic_df len ignore l'entete \n",
    "nombre_de_passagers = len(titanic_df)\n",
    "\n",
    "# Calcul du nombre de survivants\n",
    "nombre_survivants = (titanic_df[\"Survived\"] == 1).sum() # nombre_survivants = titanic_df[\"Survived\"].sum() possible aussi car les valeurs sont des 0 ou des 1 (dans python 0 == False et 1 == True)\n",
    "\n",
    "# Calcul du pourcentage de survivants\n",
    "pourcentage_survivants = (nombre_survivants/nombre_de_passagers)*100\n",
    "\n",
    "print(f\"Le pourcentage de survivant est de {round(pourcentage_survivants)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e3d2c",
   "metadata": {},
   "source": [
    "**Exercice 3 - Analyse des âges :**\n",
    "* Calculez l'âge moyen des passagers.\n",
    "* Trouvez l'âge le plus fréquent des passagers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c015cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne d'age de 30 ans.\n",
      "Moyenne d'age de 30 ans.\n",
      "L'age le plus fréquent etait de 24 ans.\n"
     ]
    }
   ],
   "source": [
    "# Methode en dur\n",
    "# On fait un tableau des ages des passagers\n",
    "tab_ages = titanic_df[\"Age\"].dropna() # dropna() permet de supprimer les cases vides et ne pas avoir de decalage avec le len() ou le sum()\n",
    "# Calcul de la somme des ages\n",
    "somme_age = tab_ages.sum()\n",
    "# Nombre de passagers ayant un age renseigné avec len \n",
    "nombre_de_passagers = len(tab_ages)\n",
    "\n",
    "# Calcul moyenne d'age \n",
    "moyenne_ages = somme_age/nombre_de_passagers\n",
    "\n",
    "print(f\"Moyenne d'age de {round(moyenne_ages)} ans.\")\n",
    "\n",
    "# pd.dataFrame.mean() calculer automatiquement le moyenne de toutes les valeurs de age sans compter les cases vides\n",
    "age_moyen = titanic_df[\"Age\"].mean()\n",
    "\n",
    "print(f\"Moyenne d'age de {round(age_moyen)} ans.\")\n",
    "\n",
    "# pd.dataFrame.mode() renvoi un tableau de ou des valeurs les plus presentes\n",
    "tab_age_le_plus_frequent = tab_ages.mode()\n",
    "\n",
    "# On recupere le premier element du tab_age_le_plus_frequent il contient la valeur avce le plus d'iterations dans le csv\n",
    "age_le_plus_frequent = tab_age_le_plus_frequent.values[0]\n",
    "\n",
    "print(f\"L'age le plus fréquent etait de {round(age_le_plus_frequent)} ans.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c28f6ff",
   "metadata": {},
   "source": [
    "**Exercice 4 - Filtrage des données :**\n",
    "* Créez un nouveau DataFrame \"survived_df\" contenant uniquement les passagers qui ont survécu.\n",
    "* Créez un nouveau DataFrame \"non_survived_df\" contenant uniquement les passagers qui n'ont pas survécu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb35507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "8            9         1       3   \n",
      "9           10         1       2   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
      "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "8      2            347742  11.1333   NaN        S  \n",
      "9      0            237736  30.0708   NaN        C  \n",
      "   PassengerId  Survived  Pclass                            Name   Sex   Age  \\\n",
      "0            1         0       3         Braund, Mr. Owen Harris  male  22.0   \n",
      "4            5         0       3        Allen, Mr. William Henry  male  35.0   \n",
      "5            6         0       3                Moran, Mr. James  male   NaN   \n",
      "6            7         0       1         McCarthy, Mr. Timothy J  male  54.0   \n",
      "7            8         0       3  Palsson, Master. Gosta Leonard  male   2.0   \n",
      "\n",
      "   SibSp  Parch     Ticket     Fare Cabin Embarked  \n",
      "0      1      0  A/5 21171   7.2500   NaN        S  \n",
      "4      0      0     373450   8.0500   NaN        S  \n",
      "5      0      0     330877   8.4583   NaN        Q  \n",
      "6      0      0      17463  51.8625   E46        S  \n",
      "7      3      1     349909  21.0750   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "# Création de dataFrame des passagers ayant survecus et de ceux ne l'ayant pas \n",
    "survived_df = titanic_df[titanic_df[\"Survived\"] == 1]\n",
    "non_survived_df = titanic_df[titanic_df[\"Survived\"] == 0]\n",
    "\n",
    "print(survived_df.head())\n",
    "print(non_survived_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9662edd",
   "metadata": {},
   "source": [
    "**Exercice 5 - Statistiques des tarifs :**\n",
    "* Trouvez le tarif minimum, maximum, moyen et médian payé par les passagers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fead7de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarif minimum de: 0.0 euros\n",
      "Tarif maximum de: 512.3292 euros\n",
      "Tarif moyen de: 32.204207968574636 euros\n",
      "Tarif median de: 14.4542 euros\n"
     ]
    }
   ],
   "source": [
    "# On créer un tableau contenant toutes les valeurs de Fare sans les cellules vides grace a dropna()\n",
    "tab_fare = titanic_df[\"Fare\"].dropna()\n",
    "\n",
    "# Calculs des tarifs min, max, moyen, et median\n",
    "min_fare = tab_fare.min()\n",
    "max_fare = tab_fare.max()\n",
    "moyenne_fare = tab_fare.mean()\n",
    "median_fare = tab_fare.median()\n",
    "\n",
    "print(f\"Tarif minimum de: {min_fare} euros\")\n",
    "print(f\"Tarif maximum de: {max_fare} euros\")\n",
    "print(f\"Tarif moyen de: {moyenne_fare} euros\")\n",
    "print(f\"Tarif median de: {median_fare} euros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10246ba",
   "metadata": {},
   "source": [
    "**Exercice 6 - Traitement des valeurs manquantes :**\n",
    "* Téléchargez le fichier CSV \"sales_data.csv\" à partir du lien suivant : https://github.com/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example/blob/master/data/sales_data.csv\n",
    "* Chargez le fichier dans un DataFrame nommé \"sales_df\".\n",
    "* Affichez le nombre de valeurs manquantes dans chaque colonne du DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb03204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                0\n",
      "Day                 0\n",
      "Month               0\n",
      "Year                0\n",
      "Customer_Age        0\n",
      "Age_Group           0\n",
      "Customer_Gender     0\n",
      "Country             0\n",
      "State               0\n",
      "Product_Category    0\n",
      "Sub_Category        0\n",
      "Product             0\n",
      "Order_Quantity      0\n",
      "Unit_Cost           0\n",
      "Unit_Price          0\n",
      "Profit              0\n",
      "Cost                0\n",
      "Revenue             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#url_2 = \"https://raw.githubusercontent.com/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example/refs/heads/master/data/sales_data.csv\"\n",
    "url_2 = \"https://raw.githubusercontent.com/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example/master/data/sales_data.csv\"\n",
    "\n",
    "sales_df = pd.read_csv(url_2)\n",
    "\n",
    "# Nombre de valeurs manquantes par colonne\n",
    "# .isnull() retourne un dataFrame de meme taille que saleds_df chaques cellules contient True si elle contient une valeur False sinon\n",
    "# True == 1 et False == 0 \n",
    "# .sum() va donc faire +0 si la cellule est remplie car isnull() sera faux donc == 0 et si la cellule est vide il prendra +1\n",
    "nombre_val_manquantes = sales_df.isnull().sum()\n",
    "\n",
    "print(nombre_val_manquantes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3238e",
   "metadata": {},
   "source": [
    "**Exercice 7 - Suppression des doublons :**\n",
    "* Téléchargez le fichier CSV \"GlobalLandTemperaturesByMajorCity.csv\" à partir du lien suivant : https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data?select=GlobalLandTemperaturesByMajorCity.csv\n",
    "* Chargez le fichier dans un DataFrame nommé \"duplicate_df\".\n",
    "* Supprimez les lignes en double du DataFrame.\n",
    "* Afficher la taille des Dataframes avant et après\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373741ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 9, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m url_3 = \u001b[33m\"\u001b[39m\u001b[33mhttps://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data?select=GlobalLandTemperaturesByMajorCity.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m duplicate_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_3\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TIM\\Documents\\Info\\Simplon\\PythonProjet\\InitiationPython\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TIM\\Documents\\Info\\Simplon\\PythonProjet\\InitiationPython\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TIM\\Documents\\Info\\Simplon\\PythonProjet\\InitiationPython\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TIM\\Documents\\Info\\Simplon\\PythonProjet\\InitiationPython\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 1 fields in line 9, saw 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url_3 = \"https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data?select=GlobalLandTemperaturesByMajorCity.csv\"\n",
    "duplicate_df = pd.read_csv(url_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
